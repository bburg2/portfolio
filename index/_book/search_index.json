[["index.html", "1 About me", " 1 About me Hi, my name is Bas and I made a website for showing my skill in working with different programming languages, these languages include R, SQL and bash. I made a few pages for showing my different types of skills. I also uploaded my resume to my website. I will keep updating this page for future usage and for tracking how I develop new skill in working with programming or other types of skills like paragliding. "],["resume.html", "2 Resume", " 2 Resume With the help of the “vitae” package I have created my own resume. The advantages of this resume is that the resume is interactive and that the resume is easily updateable for future usage. The pdf is loaded as an image on this page. It is also possible to download the pdf of my resume here: Download resume.pdf "],["project-introduction.html", "3 Project Introduction", " 3 Project Introduction 3.0.1 Introductie To learn programming better my school assigned me to a project. Here I will give a short introduction to the project. 3.0.2 about the project The project is about the erasmusladder, this is a device made for testing motor function of mice. The device does this by having sensors in every step the mice can possibly take. After the mouse walks across one of these erasmusladders data is provided by the erasmusladder. This data set cointains 120 variables, these variables are about every step a mouse can take, for example there are nine different high to low variables, these are the steps the programs sees as a misstep. The erasmusladder is usefull for testing mice under different conditions. For example one study looked at the difference between wilt-type mice and mice with degenerating purkinje cells (Vinueza Veloz et al. 2015 ). The erasmusladder also contains two different air outlets to control the moment of departure and the speed of the mice. The erasmusladder also has 2 x 37 sensors on each side of the erasmusladder wich makes is possible to output the data. The device makes a prediction if the mouse is walking at a consistant speed. If a prediction is made it will be possible to put one of the sensors down right before the mouse is there, because of this the mouse has to change his path in an instant. In some mutated mice this will not work because of a small defect in the cerebellum(van der Vaart et al. 2011). When a mouse first gets into an erasmusladder it is loaded within a box with a light signal and an airduct, there is only one way to go for the mouse which is the erasmusladder. The trail starts when there is a light signal after this the airduct starts blowing air to the other side of the erasmusladder, because of this the mouse will start to move. When the mouse moves from one box to the next box this is called a trial. After the mouse gets in the other box which is on the other side another light and air que will start after 10 to 20 seconds and the mouse has to go back. One session will take 42 trials for the mouse to be done. The erasmusladder also makes it possible to make your own protocol(Sathyanesan and Gallo 2019). One study which used the erasmusladder studied the cerebellum which plays an important role in motor learning. The study used control mice on the erasmus ladder and ßCamKII knockout mice. The study found that mice with a ßCamKII deficit have a motor performence an a motor learning deficit. This study used this data to prove that the erasmusladder works as intended an made the erasmusladder because there was no good technology to study motor performance in mice. Previous methods included grid walking, rope climbing, incline plane kimatic analysis, open-field tasks, gait analysis, measures of ground reaction forces, swimming and acceleration rotarod(Cupido, n.d.). The goal of this project is to make it easier for research to look at the data by using rmarkdown and shinyAPP to make a lot of different graphs. With shinyAPP it is possible to change which graph you want to see, and because the graphs are writen in functions it is also easy to change which aspect you want to see of the graph. To reach to goal of the project our group started working with the agile workflow which consist of sprint of two weeks. Our project group also has a scrum board where we can look what tasks we are doing to get the best possible result for the project. 3.0.3 Bibliography Cupido (n.d.); Sathyanesan and Gallo (2019); van der Vaart et al. (2011); Vinueza Veloz et al. (2015) "],["data-managment.html", "4 Data managment", " 4 Data managment One thing that is very important in data science is managing your data, if your file system is the same as the file system of your colleagues it is easier to work together on a project. I grabbed one of my old project files and started ordering the file with the use of Guerrilla analytics. In a picture below you can see how my data is ordered. folder structure "],["example-data-analysis.html", "5 Example Data Analysis", " 5 Example Data Analysis To show my skills working with basic datasets I have imported data from a C. elegans experiment and have made a few graphs based on this data. The dataset was provided by J. Louter (INT/ILC) First thing is loading in some needed libraries and the data itself library(readxl) library(tidyverse) library(here) library(DT) To import the data I used the “read_excel()” function in combination with the “here()” function. The read_excel function is used to import excel files to an object, this object can now be used to call the dataset. The here() function is used to assign the path to the file wich is imported, in this case the data is stored in the data folder. I first looked at the data that was provided, there are 360 rows of data. In the next table the data is visible. This datatable is shown using the “datatable()” function, I set the option scrollx to true, because of this it is possible to scroll through all the data. I also made a table showing some descriptive statistics of the data using the summarise function. With the help of this table I made a bar graph for better visualisation of the descriptive statistics # load the data in a datatable datatable(data, options = list(scrollx=TRUE)) # summarise some descripte statistics data_summary &lt;- data %&gt;% group_by(compName) %&gt;% summarise(mean = mean(RawData, na.rm = TRUE), sd = sd(RawData, na.rm = TRUE), minimum = min(RawData, na.rm = TRUE), maximum = max(RawData, na.rm = TRUE)) # here I loaded the data for visibility in the markdown file datatable(data_summary) %&gt;% formatRound(columns=c(&#39;mean&#39;, &#39;sd&#39;), digits=2) # plotting the descriptive statistics data_summary %&gt;% ggplot(aes(x = compName, y = mean, fill = compName)) + geom_bar(stat = &quot;identity&quot;) + theme_minimal() + geom_errorbar(aes(x = compName, ymin=mean-sd, ymax=mean+sd), width = 0.2) + labs( title = &quot;average amount of offspring per substrate&quot;, y = &quot;average ammount of offspring&quot; ) After I imported the data I looked at which classes where assigned to the variables. Most variables where imported correctly. compConcentration was imported as a character but it was a number. So I changed the class to numeric using the “as.numeric()” command. I plotted the data using ggplot, where I used the compConcentration as the x-axis and the amount of offspring as the y-axis. #compConcentration in numeric veranderen data$compConcentration &lt;- as.numeric(data$compConcentration) #grafiek maken ggplot(data = data, aes(x = compConcentration, y = RawData)) + geom_point(aes(color = compName, shape = compName))+ labs(title = &quot;Number of offspring per concentration&quot;, y = &quot;Number of offspring&quot;, x = &quot;Concentration of compound&quot;) + theme_minimal() Because I changed to compConcentration variable to numeric the x-axis data is readable, otherwise it would have been impossible to read because there would be a lot of text clutter on the x-axis. But the data is still not easy to analyse, because of this I have normalized the data in the following command with a log10 transformation. I also added some geom_jitter to make sure that the data does not overlap. #compConcentration in numeric veranderen data$compConcentration &lt;- as.numeric(data$compConcentration) #grafiek maken ggplot(data = data, aes(x = log10(compConcentration), y = RawData)) + geom_jitter(aes(color = compName, shape = compName),width = 0.08)+ labs(title = &quot;Number of offspring per concentration&quot;, y = &quot;Number of offspring&quot;, x = &quot;log10 Concentration of compound&quot;) + theme_minimal() This graph is a bit easier to read, when looking at the data it seems that on a higher concentration of 2,6-diisopropylnapthalene the C. elegans gets a lesser amount of offspring. The positive control for this experiments is Ethanol. The negative control for this experiment is S-medium. For a statistical analysis to find out the if there is indeed a difference. I would: Do a shapiro-wilk test to see if the data has a normal distribution. Next I would do a levene test to see if there is any variance within the data. If the data has a normal distribution I would do a T-test to see if there is any significant difference. The T-test would use the positive control group and another substance. In the last plot I normalised the data with the help of a the mean of the negative control. This is useful because now the negative control has no effect on the data itself. I first filtered so that it only uses data from the negative control using the “filter()” function. After this I calculated the mean ammount of offspring from the negative control experiment. Using this mean I could normalise the data and make a graph out of it. #data filteren op de negatieve controle data_negative &lt;- data %&gt;% filter(data$expType == &quot;controlNegative&quot;) #De gemiddelde berekenen van het rawdata colom van de negative controle mean_negative &lt;- mean(data_negative$RawData, na.rm = TRUE) #De data normaliseren op basis van het gemiddelde van de negatieve controle data$RawData &lt;- (data$RawData/mean_negative)*100 #grafiek maken ggplot(data = data, aes(x = log10(compConcentration), y = RawData)) + geom_jitter(aes(color = compName, shape = compName),width = 0.08)+ labs(title = &quot;Number of offspring per concentration Normalised&quot;, y = &quot;Normalised amount of offspring (%)&quot;, x = &quot;log10 Concentration of compound&quot;) + theme_minimal() In this graph you can see clearly that there is indeed a negative corralation with the 2,6-diisopropylnapthalene concentration and the amount of offspring. To know if there is a statistical significant difference a T-test could bee used. "],["reproducibility.html", "6 Reproducibility 6.1 Working with articles 6.2 Working with code from articles", " 6 Reproducibility 6.1 Working with articles One of the skills that are useful in datascience is looking at a paper and using the code in the paper to get the exact same result with the dataset. This is call reproducibility. One of the skills I have learned is to Judge an article based on its reproducibility. To show my skill on juding reproducibiility a grabbed a random article from pubmed you can find this article here This article looks at the safety of some of the first covid-19 vaccines. 6.1.1 Article References Edward E. Walsh, M.D., Robert W. Frenck, Jr., M.D., Ann R. Falsey, M.D., Nicholas Kitchin, M.D., Judith Absalon, M.D.,corresponding author Alejandra Gurtman, M.D., Stephen Lockhart, D.M., Kathleen Neuzil, M.D., Mark J. Mulligan, M.D., Ruth Bailey, B.Sc., Kena A. Swanson, Ph.D., Ping Li, Ph.D., Kenneth Koury, Ph.D., Warren Kalina, Ph.D., David Cooper, Ph.D., Camila Fontes-Garfias, B.Sc., Pei-Yong Shi, Ph.D., Özlem Türeci, M.D., Kristin R. Tompkins, B.Sc., Kirsten E. Lyke, M.D., Vanessa Raabe, M.D., Philip R. Dormitzer, M.D., Kathrin U. Jansen, Ph.D., Uğur Şahin, M.D., and William C. Gruber, M.D. 6.1.2 Criteria article Transparency Criteria Definition Response Type Response type Article Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. Binary Yes Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary Yes Data Location Where the article’s data can be accessed, either raw or processed. Found Value Email the author Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary; Found Value No Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Found by author information Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary Yes Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Yes Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary No 6.1.3 Other data The protocol, the ethics statement and the data sharing statement are founde beneath the article. 6.1.4 Extra’s about the article In general this is a good reproducible article that test the safety of multiple covid-19 vaccines with the help of a placebo trial. On of the things that made this article very reproducible is because it is covid-19 related. After the first wave of covid scientist started openly sharing their data to make the best vaccines as soon as possible. 6.2 Working with code from articles In data science it is also a useful skill to work with open science articles. These articles show their code and show how they got their data. In this part of the page I look at some code and try to reproduce it of a random article. The article is found here When looking at the code I can see that the data analysts first made some function for the purpose of creating some graphs. After this the load in the required data and look for missing value data. After cleaning the data the started making some graphs. One of the things that I look at when analysing the code is if the code hase comments describing what is being done in the code. This code has comments above every code but the comment are not always as clear as can be. After I analysed the code I copied some of the code to look at how easy it is to reproduce the article. library(here) # read data --------------------------------------------------------------- # sample 1 sample1 &lt;- read.csv(here(&quot;data&quot;,&quot;sample1_use for revision 1.csv&quot;)) sample1 &lt;- sample1[sample1$include_1,] # sample 2 sample2 &lt;- read.csv(here(&quot;data&quot;,&quot;sample2_use for revision 1.csv&quot;)) sample2 &lt;- sample2[sample2$include_1,] # combine samples for the additional analyses data &lt;- rbind(sample1, sample2) # packages ---------------------------------------------------------------- # functie inladen gg_color_hue &lt;- function(n) { hues = seq(15, 375, length = n + 1) hcl(h = hues, l = 65, c = 100)[1:n] } get_max_daily &lt;- function(x){ x &lt;- x + c(1:4, 9:12, 17:20, 25:27) x &lt;- x[x &lt; ymd(&quot;2020-04-13 UTC&quot;)] return(x) } # install.packages(&quot;lubridate&quot;) library(lubridate) # ------------------------------------------------------------------------- ## define colors c &lt;- gg_color_hue(2) col.s1 &lt;- c[1] col.s2 &lt;- c[2] # completed baseline surveys per day that are included in the study data_l2_s1 &lt;- unique(sample1[c(&quot;ID&quot;, &quot;b_baseline_ended&quot;)]) data_l2_s2 &lt;- unique(sample2[c(&quot;ID&quot;, &quot;b_baseline_ended&quot;)]) # no missings on this variable any(is.na(data_l2_s1[2])) ## [1] FALSE any(is.na(data_l2_s2[2])) ## [1] FALSE date_s1 &lt;- ymd_hms(data_l2_s1$b_baseline_ended) hour(date_s1) &lt;- 0 minute(date_s1) &lt;- 0 second(date_s1) &lt;- 0 date_s2 &lt;- ymd_hms(data_l2_s2$b_baseline_ended) hour(date_s2) &lt;- 0 minute(date_s2) &lt;- 0 second(date_s2) &lt;- 0 # number of baseline surveys completed t_date_baseline_1 &lt;- table(ymd(date_s1)) t_date_baseline_2 &lt;- table(ymd(date_s2)) t_date_baseline_1 &lt;- c(t_date_baseline_1, &quot;2020-04-12&quot; = 0) t_date_baseline_12 &lt;- rbind(t_date_baseline_1, t_date_baseline_2) # barplot sample 1 and 2 baseline participation --------------------------- layout(matrix(1:2, 1, 2, byrow = TRUE)) b &lt;- barplot(t_date_baseline_12, beside = TRUE, ylim = c(0, 800), names.arg = rep(&quot;&quot;, length(t_date_baseline_12)), col = c(col.s1, col.s2), axes = FALSE) box() axis(2, las = 2, cex.axis = 0.8) s &lt;- seq(1, 28, 7) labels &lt;- colnames(t_date_baseline_12)[s] axis(1, at = ((b[1,] + b[2,])/2)[s], labels = labels, cex.axis = 0.8) text(&quot;A&quot;, x = ((b[1,] + b[2,])/2)[2], y = 800*.9, cex = 4, col = &quot;grey&quot;) legend(x = ((b[1,] + b[2,])/2)[15], y = 800, legend = c(&quot;Sample 1&quot;, &quot;Sample 2&quot;), fill = c(col.s1, col.s2), bty = &quot;n&quot;) # dates at which a daily survey could have been completed: max_daily_dates_s1 &lt;- lapply(ymd(date_s1), get_max_daily) max_daily_dates_s2 &lt;- lapply(ymd(date_s2), get_max_daily) max_dates_s1 &lt;- do.call(&quot;c&quot;, max_daily_dates_s1) max_dates_s2 &lt;- do.call(&quot;c&quot;, max_daily_dates_s2) obtained_dates_s1 &lt;- sample1$daily_date obtained_dates_s2 &lt;- sample2$daily_date t_max_dates_s1 &lt;- table(max_dates_s1) t_max_dates_s2 &lt;- table(max_dates_s2) t_obtained_dates_s1 &lt;- table(obtained_dates_s1) t_obtained_dates_s2 &lt;- table(obtained_dates_s2) t_obtained_dates_s12 &lt;- rbind(t_obtained_dates_s1, t_obtained_dates_s2) b &lt;- barplot(t_obtained_dates_s12, beside = TRUE, ylim = c(0, 2000), names.arg = rep(&quot;&quot;, length(t_obtained_dates_s12)), col = c(col.s1, col.s2), axes = FALSE) box() axis(2, las = 2, cex.axis = 0.8) labels &lt;- colnames(t_obtained_dates_s12)[s] axis(1, at = ((b[1,] + b[2,])/2)[s], labels = labels, cex.axis = 0.8) lines(y = t_max_dates_s1, x = b[1,], type = &quot;p&quot;, pch = &quot;-&quot;, col = col.s1, cex = 1.5) lines(y = t_max_dates_s2, x = b[2,], type = &quot;p&quot;, pch = &quot;-&quot;, col = col.s2, cex = 1.5) text(&quot;B&quot;, x = ((b[1,] + b[2,])/2)[2], y = 2000*.9, cex = 4, col = &quot;grey&quot;) legend(x = ((b[1,] + b[2,])/2)[15], y = 2000, legend = c(&quot;Sample 1&quot;, &quot;Sample 2&quot;), fill = c(col.s1, col.s2), bty = &quot;n&quot;) # difference obtained and maximum ----------------------------------------- obtained_total &lt;- sum(t_obtained_dates_s12) max_reports_total &lt;- sum(c(t_max_dates_s1, t_max_dates_s2)) round(obtained_total/max_reports_total*100, 2) ## [1] 75.74 # average surveys per day ------------------------------------------------- median(colSums(t_obtained_dates_s12)) ## [1] 1468 # average number of loneliness scores per participant --------------------- s1_lst &lt;- split(sample1, f = sample1$ID) n_1 &lt;- unlist(lapply(s1_lst, function(x) sum(!is.na(x$loneliness)))) mean(n_1) ## [1] 8.150741 sd(n_1) ## [1] 3.359446 range(n_1) ## [1] 0 15 s2_lst &lt;- split(sample2, f = sample2$ID) n_2 &lt;- unlist(lapply(s2_lst, function(x) sum(!is.na(x$loneliness)))) mean(n_2) ## [1] 8.124586 sd(n_2) ## [1] 3.399153 range(n_2) ## [1] 0 15 # distribution of participants across number of daily surveys layout(1) b &lt;- barplot(table(c(n_1, n_2)), main = &quot;&quot;, ylab = &quot;valid loneliness scores&quot;, xlab = &quot;number of completed daily surveys&quot;, ylim = c(0, 900), lwd = 1.3) box(lwd = 1.3) # surveys, not valid measures n_1 &lt;- unlist(lapply(s1_lst, function(x) nrow(x))) n_2 &lt;- unlist(lapply(s2_lst, function(x) nrow(x))) mean(n_1) ## [1] 8.269769 sd(n_1) ## [1] 3.318693 range(n_1) ## [1] 1 15 mean(n_2) ## [1] 8.25207 sd(n_2) ## [1] 3.359575 range(n_2) ## [1] 1 15 b &lt;- barplot(table(c(n_1, n_2)), main = &quot;&quot;, ylab = &quot;N&quot;, xlab = &quot;number of completed daily surveys&quot;, ylim = c(0, 1000), lwd = 1.3) box(lwd = 1.3) # N and measurement occasions --------------------------------------------- sample1_lst &lt;- split(sample1, f = sample1$ID) sample2_lst &lt;- split(sample2, f = sample2$ID) length(sample1_lst) ## [1] 2428 length(sample2_lst) ## [1] 2416 length(sample1_lst) + length(sample2_lst) ## [1] 4844 nrow(sample1) ## [1] 20079 nrow(sample2) ## [1] 19937 nrow(sample1) + nrow(sample2) ## [1] 40016 # demographic variables --------------------------------------------------- names &lt;- names(sample1) variables_level_2 &lt;- grep(x = names, pattern = &quot;ID|group|b_|var_|federal&quot;, value = TRUE) data_l2_s1 &lt;- unique(sample1[variables_level_2]) data_l2_s2 &lt;- unique(sample2[variables_level_2]) ## age mean(data_l2_s1$b_demo_age_1) ## [1] 37.29462 sd(data_l2_s1$b_demo_age_1) ## [1] 14.32523 mean(data_l2_s2$b_demo_age_1) ## [1] 37.57201 sd(data_l2_s2$b_demo_age_1) ## [1] 14.24426 # generate level 2 data frame for all participants data_l2 &lt;- unique(data[c(grep(x = names(data), pattern = &quot;ID|b_demo|b_work|b_corona&quot;, value = TRUE))]) mean(data_l2$b_demo_age_1) ## [1] 37.87923 sd(data_l2$b_demo_age_1) ## [1] 14.40773 barplot(table(data_l2$b_demo_age_1), main = &quot;Distribution of age across the total sample&quot;, lwd = 1.3, ylim = c(0, 200)) box(lwd = 1.3) ## quartiles of age quantile(data_l2$b_demo_age_1) ## 0% 25% 50% 75% 100% ## 18 27 34 48 88 There are a few things I had to change within the code: I had to change the path the data was led to Some functions where loaded in another file, I copied them to the code, the functions were: “gg_color_hue” and “get_max_daily” One of the things with the output of the graphs is that they don’t contain y-axis descriptions. This article was pretty easy to reproduce the code only needed a few changes. To rate the article on reproducible I would rate it about a four on a scale of one to five. "],["plan-for-learning-on-my-own.html", "7 Plan for learning on my own 7.1 it is possible to track my progress here: https://github.com/bburg2/Learning-python", " 7 Plan for learning on my own 7.0.1 What I want to learn My school has assigned me a project to learn a skill in biological data science on my own. A minimal requirement is that I wark 32 hours on this project. To find out which skill are useful for my future interests I looked at some job applications for DNA-sequencing and analysis. Because of this I had found a few jobs that had taken my interest. These jobs all required or recommended some experience in the programming language python. Because of this learning python will be my main goal for the project. 7.0.2 My plan to learn python My plan is to first learn the basics of python using the following website https://www.learnpython.org/. I expect to learn python easily because to code R which I am experienced in looks a lot like python. After I have a basic understanding of python I want to look at DNA sequence analysis. To be specific I want to look at DNA sequences that I have generated myself with the help of the minION. In another project I am workign on I am looking at metagenomics an the sequencing of water samples. I want to look if its possible to couple my DNA sequence to a database and get the different species of bacteria that are in the water sample using python. 7.0.3 progress 7.1 it is possible to track my progress here: https://github.com/bburg2/Learning-python First I started learning the basics of python, this included: lists operators string formatting string operations conditions for and while loops After this I stared learning more complicated code: dictionaries functions classes modules Now that I acquired some basic knowledge about python I started with some basic data manipulation tools. These tools Included: numpy arrays pandas After this I started some basic DNA analysis of one of the first DNA sequences that I have generated with the help of the minION. This was a control sample containing lambda DNA. The output of sequencing a lambda library was a fasq file containing the sequencing data. I installed a tool that could analyse DNA using the following code in the terminal: pip install bioinfokit after this I made a simple script to look at the general data of the file # import bioinfokit.analys from bioinfokit.analys import fastq # load some data generated with the minION fastq_iter = fastq.fastq_reader(file=&#39;data/AIR589_pass_dbebcefe_1.fastq&#39;) # read fastq file # get sequence length sequence_len = len(sequence) # count bases a_base = sequence.count(&#39;A&#39;) c_base = sequence.count(&#39;C&#39;) t_base = sequence.count(&#39;T&#39;) g_base = sequence.count(&#39;G&#39;) # make a dictionary for the DNA data DNA = {} DNA[&quot;sequence length&quot;] = sequence_len DNA[&quot;A count&quot;] = a_base DNA[&quot;C count&quot;] = c_base DNA[&quot;T count&quot;] = t_base DNA[&quot;G count&quot;] = g_base # print the dictionary print(DNA) "],["sql-data-analysis.html", "8 SQL data analysis", " 8 SQL data analysis In this report I add three different datasets to an sql database and join the table together. With this data I make three different graphs to show my skill in R and SQL. Working with databases like SQL is a common thing in data science. Databases like SQL make it easier to work with large sizes of data. First thing is loading all the packages I use. library(dslabs) library(readr) library(tidyverse) library(here) library(DBI) library(DT) Here I load the data with the use of the “read_csv” command and the “here package” read_csv() can open an csv file and bind it to an object. As an option I used “skip = 10”, this option will skip the first 10 rows while loading the data. This because the datafiles contain metadata that is not necessary for us. The datasets that I used are flu_data, dengue_data and gapminder. flu_data contains data on the weekly cases of the flu for countries around the world. dengue_data contains data about dengue cases around the world by week gapminder contains data about health and income for 184 countries from 1960 to 2016 to look at the imported data I used the “datatable()” function to see the first 6 rows of data from each dataset. It is also possible to scroll through the data because I set the option scrollx to true. # Laden van flu data en de eerste 10 rows skippen flu_data &lt;- read_csv(here(&quot;data&quot;,&quot;flu_data.csv&quot;), skip = 10) # show the first 6 rows datatable(flu_data, options = list(scrollx=TRUE, pageLength = 6)) # Laden van denque data en de eerste 10 rows skippen dengue_data &lt;- read_csv(here(&quot;data&quot;,&quot;dengue_data.csv&quot;), skip = 10) # show the first 6 rows datatable(dengue_data, options = list(scrollx=TRUE, pageLength = 6)) # Laden van gampinder in gampinder (niet nuttig). gapminder &lt;- gapminder # show the first 6 rows datatable(gapminder, options = list(scrollx=TRUE, pageLength = 6)) Here a make the tables tidy, this for later use (is easier to work with tidy data). I also renamed the column called Date to year in the gapminder dataset. Using the “pivot_longer” function I made the dengue and flu data tidy. This means that I changed the data in a new format where there are three columns: Date, country and cases. I also changed the country column into a factor using “as.factor” # gapminder zelfde colnaam geven gapminder_tidy &lt;- gapminder %&gt;% rename(Date = year) # flu_data tidy maken flu_data_tidy &lt;- pivot_longer(data = flu_data, cols = -c(&quot;Date&quot;), names_to = &quot;country&quot;, values_to = &quot;cases&quot;) # en factor van country maken flu_data_tidy$country &lt;- as.factor(flu_data_tidy$country) # dengue_data tidy maken dengue_data_tidy &lt;- pivot_longer(data = dengue_data, cols = -c(&quot;Date&quot;), names_to = &quot;country&quot;, values_to = &quot;activity&quot;) # en factor van country maken dengue_data_tidy$country &lt;- as.factor(dengue_data_tidy$country) Where flu_data previously had 659 rows the tidydata now has 19111 rows. Where dengue_data previously had 659 rows the tidydata now has 6590 rows. After this I exported the tidy data to csv an rds files using the write_csv and write_rds fucntion. With the help of path = … I specified where I wanted to save the data and how I wanted to call it. # Oplsaan als CSV bestand write_csv(flu_data_tidy, path = here(&quot;data&quot;,&quot;flu_data_tidy.csv&quot;)) # Oplsaan als CSV bestand write_csv(dengue_data_tidy, path = here(&quot;data&quot;,&quot;dengue_data_tidy.csv&quot;)) # Oplsaan als CSV bestand write_csv(gapminder_tidy, path = here(&quot;data&quot;,&quot;gapminder_tidy.csv&quot;)) # opslaan als rds bestand write_rds(flu_data_tidy, path = here(&quot;data&quot;,&quot;flu_data_tidy.rds&quot;)) # opslaan als rds bestand write_rds(dengue_data_tidy, path = here(&quot;data&quot;,&quot;dengue_data_tidy.rds&quot;)) # opslaan als rds bestand write_rds(gapminder_tidy, path = here(&quot;data&quot;,&quot;gapminder_tidy.rds&quot;)) I used these files to import them to a SQL database using the following commands. These are the commands I used in the sql database with the program DBeaver. First I had to create a table within the database usinging “CREATE TABLE”, I named the table flu_data and added three colums: data, country and cases. I also made a primary key made up of the data and the country. With the help of COPY we can fill the newly created table with data from the csv file that we just created. FROM was used the specify the path of the data. I did the same thing for dengue_data. # en table maken CREATE TABLE flu_data ( Date VARCHAR(50), country VARCHAR(50), cases VARCHAR(50), CONSTRAINT PK_flu PRIMARY KEY (Date,country) ); # de date naar de table verplaatsen COPY flu_data FROM &#39;C:/Users/Bas/Desktop/School/Programmeren/datascience/portfolio/data/flu_data_tidy.csv&#39; WITH (FORMAT csv); # de table laten zien SELECT * FROM flu_data; # en table maken CREATE TABLE dengue_data ( Date VARCHAR(50), country VARCHAR(50), activity VARCHAR(50), CONSTRAINT PK_dengue PRIMARY KEY (Date,country) ); # de date naar de table verplaatsen COPY dengue_data FROM &#39;C:/Users/Bas/Desktop/School/Programmeren/datascience/portfolio/data/dengue_data_tidy.csv&#39; WITH (FORMAT csv); # de table laten zien SELECT * FROM dengue_data; Within DBeaver the data base now looked like this: Here I connect to the SQL database and inspect the database with the help of R. With the help of “dbConnect” I can load the database in an object called “con”. within this function I need to specify: dbname: The name of the database host: where the database is hosted, on my own pc in this case port: the connection port user: the username password: the password, in this example a bad password is used. with “dbListTables()” I show all tables that are seen (can also be seen in the “database” picture above) with “dbListField()” I show all fields within “flu_data” in this case You can also put SQL language in R by using “dbGetQuery()” I used this to show the whole flu_data table. To only show the first 6 rows I put the “head()” function around it. To disconnect from the database I used the “dbDisconnect()” function # connect to the database con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;kaas&quot;) # laat de tables zien dbListTables(con) ## [1] &quot;flu_data&quot; &quot;dengue_data&quot; &quot;gapminder&quot; # laat de colummen in flu_data zien dbListFields(con, &quot;flu_data&quot;) ## [1] &quot;date&quot; &quot;country&quot; &quot;cases&quot; # laat het tabel flu_data zien head(dbGetQuery(con, &#39;SELECT * FROM flu_data&#39;)) ## date country cases ## 1 Date country cases ## 2 2002-12-29 Argentina NA ## 3 2002-12-29 Australia NA ## 4 2002-12-29 Austria NA ## 5 2002-12-29 Belgium NA ## 6 2002-12-29 Bolivia NA # disconnect van de database dbDisconnect(con) These are the commands I used in the sql database with the program DBeaver to create the gapminder table. I also used the COPY function again to import the data. And used CONSTRAINT to make a key based on Date and country #create gapminder table CREATE TABLE gapminder ( country VARCHAR(50), Date VARCHAR(50), infant_mortality VARCHAR(50) not null, life_expectancy VARCHAR(50) not null, fertitlity VARCHAR(50) not null, population VARCHAR(50) not null, gdp VARCHAR(50) not null, continent VARCHAR(50), region VARCHAR(50), CONSTRAINT PK_gapminder PRIMARY KEY (Date,country) ); #import the gampinder file COPY gapminder FROM &#39;C:/Users/Bas/Desktop/School/Programmeren/datascience/portfolio/data/gapminder_tidy.csv&#39; WITH (FORMAT csv); # laat de tabel zien SELECT * FROM gapminder; In the next lines of code I connected to the database again and selected the flu_data and gapminder in one table using the primary key (data, country) I saved this data in “gapminder_flu” and disconnected. gapminder_flu can now be used to create some graphs of the data. # connect to the database con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;kaas&quot;) gapminder_flu &lt;- dbGetQuery(con, &#39;select distinct * from flu_data,gapminder where flu_data.country = gapminder.country;&#39; ) # disconnect van de database dbDisconnect(con) The first graph that I made are the average amount of flu cases in the Netherlands over time. I did this by first filtering for the country Netherlands. After this I grouped the data by date using the “group_by” function, the date column didn’t have the correct class yet so I used as.Date to make it a Date class. With this dataset I could now calculate the mean amount of cases per data. I used ggplot with geom_line to create the graph. Where x = date an Y = mean_cases # filter for the Netherlands and calculate the average cases over time cases_netherlands &lt;- gapminder_flu %&gt;% filter(country == &quot;Netherlands&quot;) %&gt;% group_by(date=as.Date(date)) %&gt;% summarise(mean_cases=mean(as.numeric(cases))) # make a gg line plot netherlands_graph &lt;- cases_netherlands %&gt;% ggplot(aes(x = date, y = mean_cases)) + geom_line() + labs( title = &quot;Mean flu cases over time in the netherlands&quot;, y = &quot;cases&quot; ) netherlands_graph In the above graph you can see a few peaks, this is probaply explained by the flu season, where a lot of people get the flu. You can also see that around 2004 they started tracking the amount of flu cases in 2004. In the next graph I decided to track the mean amount of flu cases in europian countries in 2015 I first had to filter for the year 2015, I did this with the “between()” function inside the “filter()” function. The between function required the first and the last day of the year 2015. After this I grouped the data by country and calculated the mean amount of flu cases. With this data it was possible to create another ggplot. Within the “geom_bar” function it is important to put “stat =”identity”“, this allows to code to make a bar graph with an y-axis and an x-axis. I also changed the angle of the country names to be vertical within “theme”, if this was not done the country text would clutter. # string naar date veranderen gapminder_flu$date &lt;- as.Date(gapminder_flu$date) # nieuw object maken waar het gemmidelde cases is berekend in 2015 in europa country_cases &lt;- gapminder_flu %&gt;% filter(between(date, as.Date(&quot;2015-01-01&quot;), as.Date(&quot;2015-12-30&quot;))) %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% group_by(country) %&gt;% summarise(mean_cases=mean(as.numeric(cases))) # ggplot maken graph &lt;-country_cases %&gt;% ggplot(aes(x = country, y = mean_cases, fill = country_cases$country)) + geom_bar(stat = &quot;identity&quot;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+ labs( title = &quot;Average flu cases per europian country in 2015&quot; ) # ggplot in een plotly veranderen graph In the above graph you can see Austria had a lot of flu cases in 2015 In the next graph I wanted to plot the average life expectancy per europian country in 2015. I did this with the same filter function as the previous graph expect I calculated the mean life_expactancy after this. After this I plotted the graph in the same ggplot function. # string naar date veranderen gapminder_flu$date &lt;- as.Date(gapminder_flu$date) # nieuw object maken waar het gemmidelde life_expectancy is berekend in 2015 in europa country_life &lt;- gapminder_flu %&gt;% filter(between(date, as.Date(&quot;2015-01-01&quot;), as.Date(&quot;2015-12-30&quot;))) %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% group_by(country) %&gt;% summarise(mean_cases=mean(as.numeric(life_expectancy))) # ggplot maken graph_life &lt;-country_life %&gt;% ggplot(aes(x = country, y = mean_cases, fill = country_cases$country)) + geom_bar(stat = &quot;identity&quot;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+ labs( title = &quot;Average life expectancy per europian country in 2015&quot; ) # ggplot in een plotly veranderen graph_life In this last graph you can see the life expectancy of a few countries. In Europe the average of lays around 70 years old. "],["package-creation.html", "9 Package creation 9.1 description 9.2 installation", " 9 Package creation 9.1 description For a school project wich I am working on I have made a package called graphify. With the help of this package it is possible to create muliple graphs using a few function wich are included in the package 9.2 installation To visit the github page of the package you can click here You can also install the package using the following code: # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;bburg2/graphify&quot;, build_vignettes = TRUE) "],["working-with-parameters.html", "10 Working with parameters", " 10 Working with parameters One of the things that I have learned is to work with parameters in markdown files. With the help of parameters its easy to change what you are analysing. The goal of parameters is to make it readable for people but also readable for the program. For the purpose of showing my skills with markdown parameters I made an analysis based on covid-19 data. With this markdown file it is possible to change parameters. The parameters that are possible to change are: Country from which you want to see the data year in which you want to look firstmonth and lastmonth, so you can look at data between specific months continent from which you want to look at In the video beneath you can look at the easy way to change parameters Working with parameters library(readr) library(here) library(tidyverse) # loading the data to an object covid_data &lt;- read_csv(here(&quot;data&quot;, &quot;covid_data.csv&quot;)) # change the date collumn to a date class covid_data$dateRep &lt;- as.Date(covid_data$dateRep, tryFormats = c(&quot;%d/%m/%y&quot;)) # filter by country, year an month country_filter &lt;- covid_data %&gt;% filter(countriesAndTerritories == params$country &amp; year == params$year &amp; month %in% (params$firstmonth:params$lastmonth)) # make a line graph based on the filter output country_filter %&gt;% ggplot(aes(x = dateRep, y = cases)) + geom_line() + labs( title = paste(&quot;Covid cases from&quot;,params$country, &quot;in&quot;, params$year), x = &quot;Date&quot; ) # filter by year and continent and sum the amount of deaths per country country_deaths &lt;- covid_data %&gt;% group_by(countriesAndTerritories) %&gt;% filter(year == params$year, continentExp == params$continent) %&gt;% summarize(deaths = sum(deaths, na.rm = TRUE)) # make a bar graph based on the filter output country_deaths %&gt;% ggplot(aes(x = countriesAndTerritories, y = deaths)) + geom_bar(stat = &quot;identity&quot;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+ labs( title = paste(&quot;covid related deaths per country in&quot;, params$continent, &quot;in&quot;, params$year), x = &quot;countries&quot; ) "],["references.html", "11 References", " 11 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
